{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Searching Embeddings\n",
    "Once we have generated the embeddings, we want to be able to search them. In order to do this efficiently, we use `gensim`. However, to do this we first must make the embeddings in a file format that `gensim` accepts. Assuming that you have run the `create_embeddings` notebook, the code below will convert that output file, `test_embed.txt`, into the glove file format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_file = \"test_embed.txt\"\n",
    "out_file = \"test_embed_glove.txt\"\n",
    "\n",
    "vec_size = 0\n",
    "line_count = 0\n",
    "\n",
    "with open(in_file, \"r\") as in_file:\n",
    "    with open(out_file, \"w\") as out_file:\n",
    "        for line in in_file:\n",
    "            row = line.split(\"\\t\")\n",
    "            if len(row) == 3 and row[1] == \"text_embedding\":\n",
    "                line_count += 1\n",
    "\n",
    "                row_id = row[0]\n",
    "                vector = row[2].split(\",\")\n",
    "\n",
    "                if vec_size == 0:\n",
    "                    vec_size = len(vector)\n",
    "\n",
    "                reformat_vector = ['{:.3f}'.format(float(x)) for x in vector]\n",
    "                full_row = f\"{row_id} {' '.join(reformat_vector)}\\n\"\n",
    "\n",
    "                out_file.write(full_row)\n",
    "\n",
    "header_out = \"test_embed_header.txt\"\n",
    "with open(header_out, \"w\") as out_file:\n",
    "    out_file.write(f\"{line_count} {vec_size}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another conversion\n",
    "However, in order to get this into the format that we want, we have to add a header with the number of lines and the dimension of the vector. The above script outputted that, so we can do this with the following command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat test_embed_header.txt test_embed_glove.txt > test_embed_wordvec.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat test_embed_wordvec.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the file and doing searches\n",
    "Now, we want to load this file and do some searches. However, the search wouldn't be very interesting because the above file only contains one software entry. We've included the larger file below, and can use this to test out the searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gunzip software_wordvec.txt.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing sentence_transformers\n",
      "importing gensim\n",
      "loading model\n",
      "loading embeddings\n",
      "What is your query?: this is a test\n",
      "cosine similarity: 0.960, url: https://github.com/jolespin/soothsayer\n",
      "cosine similarity: 0.960, url: https://github.com/bslceb/planets\n",
      "cosine similarity: 0.956, url: https://github.com/Chiliad-Spring/TACH-Two-level-Attributed-Consistent-Hashing\n",
      "cosine similarity: 0.924, url: https://github.com/NevermoreBryce/Spoon-Knife\n",
      "cosine similarity: 0.872, url: https://github.com/TobbeTripitaka/strat2file\n",
      "cosine similarity: 0.861, url: https://github.com/GiuliaFedrizzi/swd2_2020-03-11\n",
      "cosine similarity: 0.859, url: https://github.com/unicus-skmk/test\n",
      "cosine similarity: 0.859, url: https://github.com/ppernot/SK-Ana\n",
      "cosine similarity: 0.859, url: https://github.com/TrapperTeam/Trapper\n",
      "cosine similarity: 0.859, url: https://github.com/downsj/charts\n"
     ]
    }
   ],
   "source": [
    "print(\"importing sentence_transformers\")\n",
    "from sentence_transformers import SentenceTransformer\n",
    "print(\"importing gensim\")\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "print(\"loading model\")\n",
    "model = SentenceTransformer(\"bert-base-nli-cls-token\")\n",
    "\n",
    "print(\"loading embeddings\")\n",
    "software_embeddings = KeyedVectors.load_word2vec_format(\"software_wordvec.txt\", binary=False)\n",
    "\n",
    "\n",
    "while True:\n",
    "    query = input(\"What is your query?: \")\n",
    "    embedding = model.encode([query])[0]\n",
    "    results = software_embeddings.similar_by_vector(embedding, topn=10)\n",
    "    for obj_id, similarity in results:\n",
    "        # get the github url from the object id\n",
    "        parts = obj_id.split(\"/\")\n",
    "        github_url = f\"https://github.com/{parts[1]}/{parts[2]}\"\n",
    "        \n",
    "        \n",
    "        print(f\"cosine similarity: {'{:.3f}'.format(similarity)}, url: {github_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-venv",
   "language": "python",
   "name": "local-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
